---
title: "p8105_hw2_zf2352"
author: "Zhenkun Fang"
date: "2024-09-27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
library(dplyr)
library(tidyr)
library(haven)
```

# Probelm 1

```{r, show_col_types = FALSE}
nyc_subway = 
  read_csv("/Users/rubp/Desktop/Data Sci 1/Week4/HW2/p8105_hw2_zf2352/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
           na = c("NA", ".", "")) %>% 
  janitor::clean_names() %>% 
  select(line, station_name, station_latitude, station_longitude, 
         route1, route2, route3, route4, route5, 
         entry, vending, entrance_type, ada) %>% 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))

view(nyc_subway)
```

The dataset contains subway line information, station names, latitude and longitude, the routes served, whether thereâ€™s an entrance, vending availability, entrance type (stair or elevator), and ADA compliance.

My data cleaning steps include converting all missing values to "NA", converting column names to a consistent format: lower case, replacing spaces and special characters with underscores, retaining the variables that are useful for later, and converting entry variable from character to logical variable.

The resulting dataset has 1868 rows and 13 columns.

These data is tidy except the routes served. It would be better to have a variable "route(s)" with one row for each combination of entrance and route.

Answer the following questions using these data:
1. How many distinct stations are there? Note that stations are identified both by name and by line (e.g. 125th St 8th Avenue; 125st Broadway; 125st Lenox); the distinct function may be useful here.

```{r}
distinct_stations = nyc_subway %>%
  distinct(station_name, line)

num_distinct_stations = nrow(distinct_stations)

num_distinct_stations
```

- There are 465 distinct stations.

2. How many stations are ADA compliant?
```{r}
ada_complicant = nyc_subway %>% 
  filter(ada == "TRUE")

num_adacomplicant = nrow(ada_complicant)

num_adacomplicant
```

- There are 468 stations are ADA compliant.

3. What proportion of station entrances / exits without vending allow entrance?

```{r}
allow_entrance = nyc_subway %>% 
  filter(vending == "NO") %>% 
  filter(entry == "YES")

num_allow_entrance = nrow(allow_entrance)

proportion = nrow(num_allow_entrance)/nrow(nyc_subway)

proportion
```

- The proportion is 0.

```{r, reformat}
reformat_nyc_subway = nyc_subway %>% 
  pivot_longer(
    cols = route1:route5,
    names_to = "route_number",
    values_to = "route_name",
    values_drop_na = TRUE
  )

reformat_nyc_subway
```

```{r}
Atrain = reformat_nyc_subway %>% 
  filter(route_name == "A")

num_Atrain = nrow(Atrain)

num_Atrain

Atrain_ada = Atrain %>% 
  filter(ada == TRUE)

num_Atrain_ada = nrow(Atrain_ada)

num_Atrain_ada
```

There are 273 distinct stations serve the A train. Of the stations that serve the A train, 107 stations are ADA compliant.

# Problem 2
```{r}
mrtrash_wheel = read_excel("/Users/rubp/Desktop/Data Sci 1/Week4/HW2/p8105_hw2_zf2352/202309_Trash Wheel_Collection Data.xlsx",
                         na = c("NA", ".", "")) %>% 
  janitor::clean_names() %>%
  select(-c(15, 16)) %>% 
  filter(!is.na(dumpster)) %>% 
  mutate(sports_balls = as.integer(round(sports_balls)),
         year = as.integer(year))
```

```{r}
mrtrash_wheel = mrtrash_wheel %>% 
  mutate(person = "mr_trash_wheel")

proftrash_wheel = read_excel("/Users/rubp/Desktop/Data Sci 1/Week4/HW2/p8105_hw2_zf2352/202309_Trash Wheel_Collection Data.xlsx", 
                             sheet = 2,
                             na = c("NA", ".", "")) %>% 
  janitor::clean_names() %>%
  filter(!is.na(dumpster)) %>% 
  mutate(person = "prof_trash_wheel",
         year = as.integer(year))

gwynnda_trash_wheel = read_excel("/Users/rubp/Desktop/Data Sci 1/Week4/HW2/p8105_hw2_zf2352/202309_Trash Wheel_Collection Data.xlsx", 
                             sheet = 4,
                             na = c("NA", ".", "")) %>% 
  janitor::clean_names() %>%
  filter(!is.na(dumpster)) %>% 
  mutate(person = "gwynnda_trash_wheel",
         year = as.integer(year))
```

```{r combine dataset}
trash_wheel = 
  bind_rows(mrtrash_wheel, proftrash_wheel, gwynnda_trash_wheel) %>% 
  janitor::clean_names() %>% 
  relocate(person)
```

The combined dataset has 845 observations in total. The variable "person" indicates which Trash Wheel (Mr Trash Wheel, Professor Trash Wheel, or Gwynnda Trash Wheel). The variable "dumpster" indicates the dumpter number. There are also self-explanatory variables including date of collection, amount of total litter and litter type. 

```{r}
total_weight_professor_trash_wheel = trash_wheel %>%
  filter(person == "prof_trash_wheel") %>% 
  summarise(total_weight = sum(weight_tons, na.rm = TRUE))

total_cigarette_gwynnda_trash_wheel = trash_wheel %>%
  filter(person == "gwynnda_trash_wheel",
         month == "June",
         year == "2022") %>% 
  summarise(total_number_cigarette_butts = sum(cigarette_butts, na.rm = TRUE))

print(total_weight_professor_trash_wheel)
print(total_cigarette_gwynnda_trash_wheel)
```

The total weight of trash collected by Professor Trash Wheel is 216.26 tons. The total number of cigarette butts collected by Gwynnda in June of 2022 is 18120.

# Problem 3

```{r import dataset}
bakers = read_csv("/Users/rubp/Desktop/Data Sci 1/Week4/HW2/p8105_hw2_zf2352/gbb_datasets/bakers.csv",
           na = c("NA", ".", "")) %>% 
  janitor::clean_names()

bakes = read_csv("/Users/rubp/Desktop/Data Sci 1/Week4/HW2/p8105_hw2_zf2352/gbb_datasets/bakes.csv",
           na = c("NA", ".", "", "N/A")) %>% 
  janitor::clean_names()

results = read_csv("/Users/rubp/Desktop/Data Sci 1/Week4/HW2/p8105_hw2_zf2352/gbb_datasets/results.csv",
           na = c("NA", ".", "")) %>% 
  janitor::clean_names()

colnames(results) = c("series","episode","baker","technical","result")

results = results[-c(1, 2),] %>% 
  mutate(series = as.integer(series),
         episode = as.integer(episode))

colnames(bakers) [1] = "baker"
```

```{r only first word in results dataset}
bakers = bakers %>% 
  mutate(baker = sub("^(\\S+).*", "\\1", baker))
```

```{r join datasets}
combined_dataset = bakes %>% 
  full_join(bakers, by = c("baker", "series")) %>% 
  right_join(results, by = c("baker", "series", "episode")) %>% 
  relocate(baker) %>% 
  relocate(baker_age, .after = baker) %>% 
  relocate(baker_occupation, .before = series)
```

```{r}
write.csv(combined_dataset, 
          file = "combined_dataset.csv", 
          row.names = FALSE)
```

For data cleaning, I first imported all datasets and converted missing values to NA. In the 'bakers' dataset, observations listed as "N/A" were treated accordingly. After reviewing the imported data, I noticed that the column names in the 'results' dataset had an incorrect format, so I renamed them and removed any empty rows and columns. In the 'bakers' dataset, the variable for the bakers' names is labeled "baker name," while in the other two datasets, it is simply called "baker." To join the datasets, I renamed "baker name" to "baker." Additionally, the 'bakers' dataset includes full names, whereas the other datasets only contain first names, so I modified the 'bakers' dataset to include only first names. After attempting to join the datasets, I found that the "series" and "episode" columns in the 'results' dataset were stored as characters instead of integers, so I converted them to integers.

To have the final dataset, I used 'full_join' and 'right_join' function to join 3 datasets all together. The final dataset has 1136 observations in total. I put the bakers' name at the first column. The final dataset has the data of every baker's name, age, and occupation in every series and episode with valid data in terms of signature bake, show topper, hometown, technical, and result. 

The final dataset still has some issues. First, there are remaining missing values in the dataset. Second, since the 'results' dataset contains more bakers than the other two datasets, using the left_join function would result in the loss of many observations from the 'results' dataset.

```{r create table}
star_baker_table = combined_dataset %>% 
  filter(series >= 5) %>% 
  filter(result == "STAR BAKER") %>% 
  knitr::kable()

star_baker_table
```

Some bakers repeatedly won Star Baker throughout the series, making their eventual win more predictable. For instance, Richard from Season 5 won Star Baker an impressive five times, signaling him as a clear front-runner, even though he didn't win the final. Similarly, Nadiya in Season 6 and Steph in Season 10 both had multiple Star Baker titles (Nadiya had three, Steph had four), marking them as favorites for the title. Nadiya went on to win her season, making her victory less surprising due to her consistent performance.

```{r viewership}
viewership = read_csv("/Users/rubp/Desktop/Data Sci 1/Week4/HW2/p8105_hw2_zf2352/gbb_datasets/viewers.csv",
           na = c("NA", ".", "")) %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    cols = series_1:series_10,
    names_to = "series",
    values_to = "viewership"
  )

head(viewership, 10)
```

```{r}
viewership %>% 
  filter(series == "series_1") %>% 
  summarize(avg_viewership_1 = mean(viewership, na.rm = TRUE))

viewership %>% 
  filter(series == "series_5") %>% 
  summarize(avg_viewership_5 = mean(viewership, na.rm = TRUE))
```

The average viewership in Season 1 is 2.77, the average viewership in Season 5 is 10.0393.